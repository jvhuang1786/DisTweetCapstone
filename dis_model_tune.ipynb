{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import time\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter Tuning with Crossvalidation in GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('DiSmldata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>tweet_len</th>\n",
       "      <th>cap_count</th>\n",
       "      <th>user_follower</th>\n",
       "      <th>user_favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>new mickey flower bed going make event photo l...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>193</td>\n",
       "      <td>20</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thought different yesterday mickey flower bed ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>13</td>\n",
       "      <td>638.0</td>\n",
       "      <td>195775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mickey flower bed renewed tokyo disneyland tod...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>16</td>\n",
       "      <td>157.0</td>\n",
       "      <td>4158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mickey flower bed back tdrnow</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>83.0</td>\n",
       "      <td>8334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>god has face changed</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>882.0</td>\n",
       "      <td>41436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87503</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>hurtado help cypress overpower pacifica finish...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>124</td>\n",
       "      <td>11</td>\n",
       "      <td>572.0</td>\n",
       "      <td>124266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87504</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>oct agricultural counsellor attended award cer...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>256</td>\n",
       "      <td>24</td>\n",
       "      <td>82.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87506</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>being single move</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>788.0</td>\n",
       "      <td>22525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>time pose hard without laughing disneyland def...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87509</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>who let yall damn cute wtf</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>550.0</td>\n",
       "      <td>15303.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87510 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hash_count  emoji_count  \\\n",
       "0               1            0   \n",
       "2               1            0   \n",
       "5               0            0   \n",
       "17              1            0   \n",
       "19              0            0   \n",
       "...           ...          ...   \n",
       "87503           3            0   \n",
       "87504           1            1   \n",
       "87506           0            0   \n",
       "87508           0            0   \n",
       "87509           0            2   \n",
       "\n",
       "                                              clean_text  label  anger  \\\n",
       "0      new mickey flower bed going make event photo l...      2      0   \n",
       "2      thought different yesterday mickey flower bed ...      1      0   \n",
       "5      mickey flower bed renewed tokyo disneyland tod...      1      0   \n",
       "17                         mickey flower bed back tdrnow      1      0   \n",
       "19                                  god has face changed      1      0   \n",
       "...                                                  ...    ...    ...   \n",
       "87503  hurtado help cypress overpower pacifica finish...      5      0   \n",
       "87504  oct agricultural counsellor attended award cer...      5      1   \n",
       "87506                                  being single move      4      0   \n",
       "87508  time pose hard without laughing disneyland def...      3      0   \n",
       "87509                         who let yall damn cute wtf      4      1   \n",
       "\n",
       "       anticipation  disgust  fear  joy  sadness  surprise  trust  punc_count  \\\n",
       "0                 0        0     0    0        0         0      0           9   \n",
       "2                 1        0     0    0        0         0      0          10   \n",
       "5                 0        0     0    0        0         0      0          12   \n",
       "17                0        0     0    0        0         0      0          10   \n",
       "19                1        0     1    1        0         0      1           0   \n",
       "...             ...      ...   ...  ...      ...       ...    ...         ...   \n",
       "87503             0        0     0    0        0         0      0          11   \n",
       "87504             2        0     1    4        1         4      4           9   \n",
       "87506             0        0     0    0        0         0      0           0   \n",
       "87508             1        0     0    1        0         0      0           2   \n",
       "87509             0        1     0    0        0         0      0           0   \n",
       "\n",
       "       tweet_len  cap_count  user_follower  user_favorite  \n",
       "0            193         20         1214.0          982.0  \n",
       "2            115         13          638.0       195775.0  \n",
       "5            119         16          157.0         4158.0  \n",
       "17            72         12           83.0         8334.0  \n",
       "19            29          3          882.0        41436.0  \n",
       "...          ...        ...            ...            ...  \n",
       "87503        124         11          572.0       124266.0  \n",
       "87504        256         24           82.0           16.0  \n",
       "87506         23          1          788.0        22525.0  \n",
       "87508        104          5           24.0         1819.0  \n",
       "87509         31          1          550.0        15303.0  \n",
       "\n",
       "[87510 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ENGLISH_STOP_WORDS.union(['disneyland','tokyo','disney', 'im', 'tdrnow','paris','california','amp','disneysea','got',\n",
    "                                         'ºc', 'ºf', 'ºoº','𝗧𝗵𝗲','くまのプーさん', 'ディズニー', 'ディズニーシー','ディズニーハロウィーン',\n",
    "                                         'ディズニーランド', 'ディズニー好きと繋がりたい', 'フェスティバルオブミスティーク', 'マルマン',\n",
    "                                         'ㅋㅋㅋ', '場所', '更新', '月released', '東京ディズニーシー', '東京ディズニーランド', '東京ディズニーリゾート',\n",
    "                                         '香港迪士尼樂園', 'ºº', 'hong', 'kong',\"disneylandresort\", \"disneyland\", \"disneyresort\",\n",
    "                                          \"californiaadventure\",'downtowndisney','disneyanaheim','disneylandanaheim',\n",
    "                                          'disneycalifornia','californiadisney','disneysea', 'disneytokyo', 'disneytokyoresort', \n",
    "                                          'tokyodisney','tokyodisneyresort', 'tokyodisneyland','東京ディズニーランド', 'ディズニーランド',\n",
    "                                          '東京ディズニーシー', 'ズニーシー', 'tdr_now', 'tdr_md','tdr','dca','dl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1013 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hash_count  emoji_count  anger  anticipation  disgust  fear  joy  sadness  \\\n",
       "0           1            0      0             0        0     0    0        0   \n",
       "1           0            0      0             0        1     1    0        2   \n",
       "2           1            0      0             1        0     0    0        0   \n",
       "3           8            0      0             0        0     0    0        0   \n",
       "4           2            0      0             0        0     0    0        0   \n",
       "\n",
       "   surprise  trust  ...  990  991  992  993  994  995  996  997  998  999  \n",
       "0         0      0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "1         0      0  ...    0    1    0    0    0    0    0    0    0    0  \n",
       "2         0      0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "3         0      0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "4         0      0  ...    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 1013 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['label']\n",
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(stop_words = my_stop_words, max_features = 1000)\n",
    "X_tfidf = tfidf_vect.fit_transform(df['clean_text'])\n",
    "X_tfidf_feat = pd.concat([df[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words = my_stop_words, max_features = 1000)\n",
    "X_count = count_vect.fit_transform(df['clean_text'])\n",
    "X_count_feat = pd.concat([df[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']], pd.DataFrame(X_count.toarray())], axis=1)\n",
    "\n",
    "X_count_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "from sklearn import preprocessing\n",
    "X_count_scaled = preprocessing.scale(X_count_feat)\n",
    "X_tfidf_scaled = preprocessing.scale(X_tfidf_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 29504), (1, 29504), (2, 29504), (3, 29504), (4, 29504), (5, 29504)]\n"
     ]
    }
   ],
   "source": [
    "#Balance train Data \n",
    "ros = RandomOverSampler(random_state=77)\n",
    "X_ros_count, y_ros = ros.fit_resample(X_count_scaled, y)\n",
    "X_ros_tfidf, y_ros = ros.fit_resample(X_tfidf_scaled, y)\n",
    "print(sorted(Counter(y_ros).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 29504), (1, 29504), (2, 29504), (3, 29504), (4, 29504), (5, 29504)]\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=77)\n",
    "X_count_r, y_r = ros.fit_resample(X_count_feat, y)\n",
    "X_tfidf_r, y_r = ros.fit_resample(X_tfidf_feat, y)\n",
    "print(sorted(Counter(y_r).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min 12s, sys: 8.03 s, total: 55min 20s\n",
      "Wall time: 1h 41min 42s\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class = 'multinomial')\n",
    "\n",
    "#Hyper tuning\n",
    "max_iter = [20, 50, 100, 300, 500]\n",
    "solver = ['newton-cg', 'sag', 'saga']\n",
    "C = [0.001, 0.01, 0.05, 0.1, 1, 3, 6, 10]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(max_iter=max_iter, solver = solver, C=C)\n",
    "\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time lr_cv_fit = clf.fit(X_ros_count, y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'newton-cg', 'max_iter': 300, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "print(lr_cv_fit.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6684912980805369"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 24s, sys: 2.73 s, total: 18min 27s\n",
      "Wall time: 1h 58min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alisha/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class = 'multinomial')\n",
    "\n",
    "#Hyper tuning \n",
    "max_iter = [20, 50, 100, 300, 500]\n",
    "solver = ['newton-cg', 'sag', 'saga']\n",
    "C = [0.001, 0.01, 0.05, 0.1, 1, 3, 6, 10]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C = C, max_iter = max_iter, solver = solver)\n",
    "\n",
    "clf = RandomizedSearchCV(lr, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time lr_cv_fit_t = clf.fit(X_ros_tfidf, y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'saga', 'max_iter': 300, 'C': 0.05}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv_fit_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv_fit_t.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6641642036144183"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv_fit_t.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multinomial Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 13.6 s, total: 1min 37s\n",
      "Wall time: 3h 59min 2s\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "\n",
    "alpha = np.linspace(0.05,1,1000)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha = alpha)\n",
    "\n",
    "clf = GridSearchCV(nb_clf, hyperparameters, cv=5, n_jobs=-1)\n",
    "%time nb_cv_fit = clf.fit(X_count_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5839095336145423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 s, sys: 7.94 s, total: 37.4 s\n",
      "Wall time: 49min 8s\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "\n",
    "alpha = np.linspace(0.05,1,1000)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha = alpha)\n",
    "\n",
    "clf = GridSearchCV(nb_clf, hyperparameters, cv=5, n_jobs=-1)\n",
    "%time nb_cv_fit_t = clf.fit(X_tfidf_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.054754754754754754}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.054754754754754754, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit_t.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6006586855067381"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_cv_fit_t.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 10s, sys: 6.68 s, total: 19min 17s\n",
      "Wall time: 2h 42min 28s\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier Countvectorizer\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators': range(50,1000,100), \n",
    "    'max_depth': [5,50,100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(rf_clf, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time rf_cv_fit = clf.fit(X_count_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 650, 'max_features': 'log2', 'max_depth': 500}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=500, max_features='log2',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=650,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9142885741441684"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 53s, sys: 7.31 s, total: 29min\n",
      "Wall time: 2h 58min 22s\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier TFIDF\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators': range(50,1000,100), \n",
    "    'max_depth': [5,50,100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(rf_clf, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time rf_cv_fit_t = clf.fit(X_tfidf_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 350, 'max_features': 'sqrt', 'max_depth': 500}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=500, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=350,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit_t.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105150782707406"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_fit_t.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Nearest Neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 2.47 s, total: 3.57 s\n",
      "Wall time: 1h 17min 18s\n"
     ]
    }
   ],
   "source": [
    "#KNN CountVectorizer\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_neighbors': range(5,50,10), \n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(knn, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time knn_cv_fit = clf.fit(X_count_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance', 'n_neighbors': 15, 'algorithm': 'brute'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314040282553842"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 4.29 s, total: 7.49 s\n",
      "Wall time: 1h 1min 19s\n"
     ]
    }
   ],
   "source": [
    "#KNN TFIDF \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_neighbors': range(5,50,10), \n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(knn, hyperparameters, cv=5, n_jobs=-1, \n",
    "                         n_iter = 10, random_state = 77)\n",
    "%time knn_cv_fit_t = clf.fit(X_tfidf_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weights': 'distance', 'n_neighbors': 15, 'algorithm': 'brute'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit_t.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205072643361081"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_cv_fit_t.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the Best Model lets test now to see which models we should further tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['hash_count', 'emoji_count','clean_text',\n",
    "                                                        'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']], df['label'],test_size =0.20, random_state = 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count Vectorizer ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer Document term matrix\n",
    "\n",
    "count_vecto = CountVectorizer(stop_words = my_stop_words, max_features = 1000)\n",
    "count_vecto_fit = count_vecto.fit(X_train['clean_text'])\n",
    "\n",
    "count_train = count_vecto_fit.transform(X_train['clean_text'])\n",
    "count_test = count_vecto_fit.transform(X_test['clean_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']].reset_index(drop=True), \n",
    "           pd.DataFrame(count_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']].reset_index(drop=True), \n",
    "           pd.DataFrame(count_test.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70008, 1013)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 23522), (1, 23522), (2, 23522), (3, 23522), (4, 23522), (5, 23522)]\n"
     ]
    }
   ],
   "source": [
    "#Balance train Data \n",
    "ros = RandomOverSampler(random_state=77)\n",
    "X_resampled_ros, y_resampled_ros = ros.fit_resample(X_train_vect.values, y_train.values)\n",
    "print(sorted(Counter(y_resampled_ros).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5982), (1, 5982), (2, 5982), (3, 5982), (4, 5982), (5, 5982)]\n"
     ]
    }
   ],
   "source": [
    "# #Balance test data\n",
    "# X_resampled_ros_test, y_resampled_ros_test = ros.fit_resample(X_test_vect.values, y_test.values)\n",
    "# print(sorted(Counter(y_resampled_ros_test).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 304.388 / Predict time: 2.32 ---- Precision: 0.716 / Recall: 0.656 / Accuracy: 0.738\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Count vectorizer \n",
    "\n",
    "#{'n_estimators': 650, 'max_features': 'log2', 'max_depth': 500}\n",
    "\n",
    "\n",
    "\n",
    "#Instantiate our model \n",
    "rf = RandomForestClassifier(n_estimators = 650, max_depth = 500, max_features = 'log2',\n",
    "                            n_jobs=-1)\n",
    "\n",
    "#Model Fit \n",
    "start = time.time()\n",
    "rf.fit(X_resampled_ros, y_resampled_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = rf.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07264998189604532, 'tweet_len'),\n",
       " (0.04641853216744782, 'cap_count'),\n",
       " (0.04272377734677494, 'punc_count'),\n",
       " (0.038893919132389765, 'hash_count'),\n",
       " (0.02146655519317955, 'sadness'),\n",
       " (0.018894953953547726, 'joy'),\n",
       " (0.017807906473101077, 'fear'),\n",
       " (0.016860613435313247, 'emoji_count'),\n",
       " (0.014306119628743563, 'anger'),\n",
       " (0.014305397515462942, 'anticipation'),\n",
       " (0.012611158225566781, 'trust'),\n",
       " (0.012546505297928749, 'disgust'),\n",
       " (0.009523934533368706, 172),\n",
       " (0.008760837448836236, 'surprise'),\n",
       " (0.008609675090072008, 470),\n",
       " (0.0050045463033912265, 547),\n",
       " (0.004917858903756468, 937),\n",
       " (0.004756201692522895, 496),\n",
       " (0.004426082061033796, 318),\n",
       " (0.004410321722824947, 182)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "(sorted(zip(importances, X_train_vect.columns), reverse=True))[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 12.957 / Predict time: 0.075 ---- Precision: 0.541 / Recall: 0.602 / Accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Bayes\n",
    "\n",
    "#Instantiate our model \n",
    "nb_clf = MultinomialNB(alpha = 0.05)\n",
    "\n",
    "#Train our Model \n",
    "start = time.time()\n",
    "nb_clf.fit(X_resampled_ros, y_resampled_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = nb_clf.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), \n",
    "    round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logitstic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "from sklearn import preprocessing\n",
    "X_train_scaled = preprocessing.scale(X_resampled_ros)\n",
    "X_test_scaled = preprocessing.scale(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 266.405 / Predict time: 0.017 ---- Precision: 0.535 / Recall: 0.617 / Accuracy: 0.533\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "#{'solver': 'newton-cg', 'max_iter': 300, 'C': 1}\n",
    "\n",
    "\n",
    "\n",
    "#Instantiate our model\n",
    "lr = LogisticRegression(max_iter = 300, solver = 'newton-cg', multi_class = 'multinomial', \n",
    "                        C = 1)\n",
    "\n",
    "#Train our Model\n",
    "start = time.time()\n",
    "lr.fit(X_train_scaled, y_resampled_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), \n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.008 / Predict time: 59.804 ---- Precision: 0.354 / Recall: 0.38 / Accuracy: 0.386\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "#{'weights': 'distance', 'n_neighbors': 15, 'algorithm': 'brute'}\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Instantiate our model \n",
    "knn = KNeighborsClassifier(n_neighbors = 15, weights = 'distance', algorithm = 'brute')\n",
    "\n",
    "#Model Fit \n",
    "start = time.time()\n",
    "knn.fit(X_resampled_ros, y_resampled_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = knn.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TFIDF ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF vectorizer Document term matrix\n",
    "\n",
    "tfidf_vecto = TfidfVectorizer(stop_words = my_stop_words, max_features = 1000)\n",
    "tfidf_vecto_fit = tfidf_vecto.fit(X_train['clean_text'])\n",
    "\n",
    "tfidf_train = tfidf_vecto_fit.transform(X_train['clean_text'])\n",
    "tfidf_test = tfidf_vecto_fit.transform(X_test['clean_text'])\n",
    "\n",
    "X_train_t_vect = pd.concat([X_train[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_t_vect = pd.concat([X_test[['hash_count', 'emoji_count', 'anger','anticipation','disgust','fear',\n",
    "                                                       'joy', 'sadness', 'surprise', 'trust', 'punc_count',\n",
    "                                                       'tweet_len','cap_count']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 23522), (1, 23522), (2, 23522), (3, 23522), (4, 23522), (5, 23522)]\n"
     ]
    }
   ],
   "source": [
    "#Balance train Data \n",
    "ros = RandomOverSampler(random_state=77)\n",
    "X_tfidf_ros, y_tfidf_ros = ros.fit_resample(X_train_t_vect.values, y_train.values)\n",
    "print(sorted(Counter(y_tfidf_ros).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Balance test data\n",
    "# X_tfidf_ros_test, y_tfidf_ros_test = ros.fit_resample(X_test_vect.values, y_test.values)\n",
    "# print(sorted(Counter(y_tfidf_ros_test).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 262.597 / Predict time: 1.098 ---- Precision: 0.682 / Recall: 0.644 / Accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "#{'n_estimators': 350, 'max_features': 'sqrt', 'max_depth': 500}\n",
    "\n",
    "\n",
    "#Instantiate our model \n",
    "rf = RandomForestClassifier(n_estimators = 350, max_depth = 500, max_features = 'sqrt',\n",
    "                            n_jobs=-1)\n",
    "\n",
    "#Model Fit \n",
    "start = time.time()\n",
    "rf.fit(X_tfidf_ros, y_tfidf_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = rf.predict(X_test_t_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.219 / Predict time: 0.042 ---- Precision: 0.508 / Recall: 0.571 / Accuracy: 0.573\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Bayes\n",
    "#{'alpha': 0.054754754754754754}\n",
    "\n",
    "\n",
    "\n",
    "#Instantiate our model \n",
    "nb_clf = MultinomialNB(alpha = 0.054754754754754754)\n",
    "\n",
    "#Train our Model \n",
    "start = time.time()\n",
    "nb_clf.fit(X_tfidf_ros, y_tfidf_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = nb_clf.predict(X_test_t_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), \n",
    "    round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.122 / Predict time: 48.819 ---- Precision: 0.346 / Recall: 0.38 / Accuracy: 0.384\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#{'weights': 'distance', 'n_neighbors': 15, 'algorithm': 'brute'}\n",
    "\n",
    "\n",
    "#Instantiate our model \n",
    "knn = KNeighborsClassifier(n_neighbors = 15, weights = 'distance', algorithm = 'brute')\n",
    "\n",
    "#Model Fit \n",
    "start = time.time()\n",
    "knn.fit(X_tfidf_ros, y_tfidf_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = knn.predict(X_test_t_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "from sklearn import preprocessing\n",
    "X_train_scaled_t = preprocessing.scale(X_tfidf_ros)\n",
    "X_test_scaled_t = preprocessing.scale(X_test_t_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 869.668 / Predict time: 0.013 ---- Precision: 0.545 / Recall: 0.635 / Accuracy: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alisha/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression \n",
    "#{'solver': 'saga', 'max_iter': 300, 'C': 0.05}\n",
    "\n",
    "\n",
    "#Instantiate our model\n",
    "lr = LogisticRegression(max_iter = 300, solver = 'saga', multi_class = 'multinomial', \n",
    "                        C = 0.05)\n",
    "\n",
    "#Train our Model\n",
    "start = time.time()\n",
    "lr.fit(X_train_scaled_t, y_tfidf_ros)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "#Model Predict \n",
    "start = time.time()\n",
    "y_pred = lr.predict(X_test_scaled_t)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "#Model Scoring \n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), \n",
    "    round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best model two models and then we will do max feature vs min_df, max_df vs with ngrams and without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go with countvectorizer Multinomial Naive Bayes and Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
